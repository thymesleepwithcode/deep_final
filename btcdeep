# Robust downloader + feature cell (แทน cell เก่า)
!pip install -q yfinance ta

import yfinance as yf
import pandas as pd
import numpy as np
import ta
from pathlib import Path
import torch, random
random.seed(42); np.random.seed(42); torch.manual_seed(42)
OUTDIR = Path("./btc_experiment"); OUTDIR.mkdir(exist_ok=True)

def canonicalize_columns(df):
    """
    Make column names simple strings.
    If df.columns is MultiIndex (e.g. ('BTC-USD','Open')), join into 'BTC-USD_Open'.
    Otherwise return as-is.
    """
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = ['_'.join([str(c) for c in col]).strip() for col in df.columns.values]
    else:
        # ensure string names
        df.columns = [str(c) for c in df.columns]
    return df

def pick_column(df, patterns):
    """
    Return first column name in df.columns that matches any substring in patterns (case-insensitive).
    patterns: list of substrings
    Returns None if not found.
    """
    cols = list(df.columns)
    lower_cols = [c.lower() for c in cols]
    for p in patterns:
        p = p.lower()
        for i, lc in enumerate(lower_cols):
            if p in lc:
                return cols[i]
    return None

def download_and_prepare(ticker="BTC-USD", start="2016-01-01", end=None, interval="1d"):
    # download with auto_adjust=False to keep 'Adj Close' when available
    raw = yf.download(ticker, start=start, end=end, interval=interval, progress=False, auto_adjust=False)
    if raw.empty:
        raise RuntimeError("No data returned from yfinance for ticker " + ticker)
    raw = canonicalize_columns(raw)

    # try to find columns by matching patterns
    col_open  = pick_column(raw, ['open'])
    col_high  = pick_column(raw, ['high'])
    col_low   = pick_column(raw, ['low'])
    # prefer 'adj close' but fall back to 'close'
    col_adj   = pick_column(raw, ['adj close', 'adj_close', 'adjclose', 'adjusted close'])
    if col_adj is None:
        col_adj = pick_column(raw, ['close'])
        use_close_for_adj = True
    else:
        use_close_for_adj = False
    col_vol   = pick_column(raw, ['volume', 'vol'])
    # also keep Close separately if exists
    col_close = pick_column(raw, ['close']) if col_adj != pick_column(raw, ['close']) else col_adj

    # Check found columns
    found = {'open':col_open, 'high':col_high, 'low':col_low, 'adj':col_adj, 'close':col_close, 'vol':col_vol}
    missing = [k for k,v in found.items() if v is None and k in ['open','high','low','adj','vol']]
    if missing:
        raise RuntimeError(f"Could not find columns {missing} in downloaded data. Available columns: {list(raw.columns)[:20]}")

    # build a canonical dataframe with desired column names
    df = pd.DataFrame(index=raw.index)
    df['Open']      = pd.to_numeric(raw[col_open], errors='coerce')
    df['High']      = pd.to_numeric(raw[col_high], errors='coerce')
    df['Low']       = pd.to_numeric(raw[col_low], errors='coerce')
    df['Close']     = pd.to_numeric(raw[col_close], errors='coerce') if col_close is not None else pd.to_numeric(raw[col_adj], errors='coerce')
    # canonical Adj_Close: use adj if present, otherwise use Close
    df['Adj_Close'] = pd.to_numeric(raw[col_adj], errors='coerce') if not use_close_for_adj else df['Close']
    df['Volume']    = pd.to_numeric(raw[col_vol], errors='coerce') if col_vol is not None else np.nan

    # drop rows with any NaN in these base columns
    df = df.dropna(subset=['Adj_Close'])
    return df

print("Downloading BTC-USD...")
df = download_and_prepare("BTC-USD", start="2016-01-01")
print("Rows:", len(df))
display(df.head())

# ---------- feature engineering (robust) ----------
def add_features(df):
    df = df.copy()
    df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')
    # returns
    df['return_1'] = df['Adj_Close'].pct_change()
    for lag in [1,2,3,5,7]:
        df[f'ret_lag_{lag}'] = df['return_1'].shift(lag)
    # moving averages and diffs
    for w in [3,5,10,20,50]:
        ma = df['Adj_Close'].rolling(window=w).mean()
        df[f'ma_{w}'] = ma
        df[f'ma_{w}_diff'] = np.where(ma!=0, df['Adj_Close'] / ma - 1.0, np.nan)
    # volatility
    df['vol_10'] = df['return_1'].rolling(10).std()
    df['vol_20'] = df['return_1'].rolling(20).std()
    # indicators
    df['rsi14'] = ta.momentum.rsi(df['Adj_Close'], window=14)
    df['macd'] = ta.trend.macd(df['Adj_Close'])
    df['macd_sig'] = ta.trend.macd_signal(df['Adj_Close'])
    # volume pct change
    if 'Volume' in df.columns:
        df['vol_change'] = df['Volume'].pct_change()
    else:
        df['vol_change'] = np.nan
    # dow
    df['dow'] = df.index.dayofweek
    # forward return and label
    df['fwd_return_1'] = df['Adj_Close'].pct_change().shift(-1)
    df['label'] = (df['fwd_return_1'] > 0).astype(int)
    df['ema_12'] = ta.trend.ema_indicator(df['Adj_Close'], window=12)
    df['ema_26'] = ta.trend.ema_indicator(df['Adj_Close'], window=26)
    df['bollinger_hband'] = ta.volatility.bollinger_hband(df['Adj_Close'])
    df['bollinger_lband'] = ta.volatility.bollinger_lband(df['Adj_Close'])
    df['momentum'] = df['Adj_Close'] / df['Adj_Close'].shift(3) - 1
    return df

df = add_features(df)
df = df.dropna().copy()
print("After features:", df.shape)
display(df.head())


# 4) Time-series train/val/test split
def time_split(df, train_frac=0.7, val_frac=0.15):
    n = len(df)
    n_train = int(n * train_frac)
    n_val = int(n * (train_frac + val_frac))
    train = df.iloc[:n_train].copy()
    val = df.iloc[n_train:n_val].copy()
    test = df.iloc[n_val:].copy()
    return train, val, test

train_df, val_df, test_df = time_split(df, 0.7, 0.15)
print("Split sizes (rows):", len(train_df), len(val_df), len(test_df))

# 5) Select features & scale (use train stats only)
EXCLUDE = ['label','fwd_return_1','Adj_Close','Open','High','Low']  # exclude raw price columns
FEATURES = [c for c in df.columns if c not in EXCLUDE]
print("Using features (count):", len(FEATURES))
print(FEATURES)

scaler = StandardScaler()
X_train = scaler.fit_transform(train_df[FEATURES])
X_val   = scaler.transform(val_df[FEATURES])
X_test  = scaler.transform(test_df[FEATURES])

y_train = train_df['label'].values
y_val   = val_df['label'].values
y_test  = test_df['label'].values

print("Class balance (train/val/test):", np.bincount(y_train), np.bincount(y_val), np.bincount(y_test))

# 6) Baseline: Logistic Regression
print("\n=== Baseline: Logistic Regression ===")
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
yhat_lr = lr.predict(X_test)
acc_lr = accuracy_score(y_test, yhat_lr)
print("Logistic Test accuracy: %.4f" % acc_lr)
print(classification_report(y_test, yhat_lr, digits=4))

# 7) Sequence dataset for LSTM (window -> predict next)
SEQ_LEN = 30

class SeqDataset(Dataset):
    def __init__(self, X, y, seq_len):
        self.X = X
        self.y = y
        self.seq_len = seq_len
    def __len__(self):
        return max(0, len(self.X) - self.seq_len)
    def __getitem__(self, idx):
        x = self.X[idx: idx + self.seq_len]            # (seq_len, features)
        y = self.y[idx + self.seq_len]                 # label at next step
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)

# construct seq datasets carefully to preserve time order
train_ds = SeqDataset(X_train, y_train, SEQ_LEN)
# for val and test we concatenate a prefix of previous partition to allow full sequences
val_prefix = X_train[-SEQ_LEN:]
val_X = np.vstack([val_prefix, X_val])
val_y = np.concatenate([y_train[-SEQ_LEN:], y_val])
val_ds = SeqDataset(val_X, val_y, SEQ_LEN)

test_prefix = X_val[-SEQ_LEN:]
test_X = np.vstack([test_prefix, X_test])
test_y = np.concatenate([y_val[-SEQ_LEN:], y_test])
test_ds = SeqDataset(test_X, test_y, SEQ_LEN)

BATCH_SIZE = 64
train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)
test_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)

print("Seq dataset sizes:", len(train_ds), len(val_ds), len(test_ds))

# 8) LSTM model
class LSTMClassifier(nn.Module):
    def __init__(self, n_features, hidden_size=128, num_layers=2, dropout=0.3):
        super().__init__()
        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 2)
        )
    def forward(self, x):
        out, _ = self.lstm(x)            # out: (B, T, H)
        h = out[:, -1, :]                # last timestep
        return self.fc(h)

n_features = X_train.shape[1]
model = LSTMClassifier(n_features, hidden_size=64, num_layers=1).to(DEVICE)
loss_fn = nn.CrossEntropyLoss()
opt = torch.optim.Adam(model.parameters(), lr=1e-3)

# 9) Training loop with val monitoring (save best)
def train_model(model, opt, loss_fn, train_dl, val_dl, epochs=50, save_path=OUTDIR/"best_btc_lstm.pth"):
    best_val_acc = 0.0
    history = {"tr_loss":[], "val_loss":[], "tr_acc":[], "val_acc":[]}
    for ep in range(1, epochs+1):
        model.train()
        running_loss = 0.0; correct = 0; total = 0
        for xb, yb in train_dl:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            opt.zero_grad()
            logits = model(xb)
            loss = loss_fn(logits, yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            opt.step()
            running_loss += loss.item() * yb.size(0)
            preds = logits.argmax(1)
            correct += (preds == yb).sum().item()
            total += yb.size(0)
        tr_loss = running_loss / total if total>0 else 0.0
        tr_acc = correct / total if total>0 else 0.0

        # validation
        model.eval()
        vloss = 0.0; vcorrect = 0; vtotal = 0
        with torch.no_grad():
            for xb, yb in val_dl:
                xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                logits = model(xb)
                loss = loss_fn(logits, yb)
                vloss += loss.item() * yb.size(0)
                vpred = logits.argmax(1)
                vcorrect += (vpred == yb).sum().item()
                vtotal += yb.size(0)
        val_loss = vloss / vtotal if vtotal>0 else 0.0
        val_acc = vcorrect / vtotal if vtotal>0 else 0.0

        history["tr_loss"].append(tr_loss); history["val_loss"].append(val_loss)
        history["tr_acc"].append(tr_acc); history["val_acc"].append(val_acc)

        print(f"[Epoch {ep:02d}] tr_loss={tr_loss:.4f} tr_acc={tr_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), str(save_path))
    print("Best val acc:", best_val_acc)
    return history

hist = train_model(model, opt, loss_fn, train_dl, val_dl, epochs=50)

# 10) Load best model and evaluate on test set
best_path = OUTDIR/"best_btc_lstm.pth"
if best_path.exists():
    model.load_state_dict(torch.load(best_path))
model.eval()
y_true=[]; y_pred=[]
with torch.no_grad():
    for xb, yb in test_dl:
        xb = xb.to(DEVICE)
        logits = model(xb)
        preds = logits.argmax(1).cpu().numpy()
        y_pred.extend(preds); y_true.extend(yb.numpy())

acc_lstm = accuracy_score(y_true, y_pred)
print("\n=== Final Evaluation ===")
print("Logistic Test accuracy: %.4f" % acc_lr)
print("LSTM Test accuracy: %.4f" % acc_lstm)
print("\nClassification report (LSTM):")
print(classification_report(y_true, y_pred, digits=4))

# 11) Plot training curves and confusion matrix
OUTDIR.mkdir(exist_ok=True)
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(hist['tr_loss'], label='train_loss'); plt.plot(hist['val_loss'], label='val_loss')
plt.xlabel('epoch'); plt.title('Loss'); plt.legend()
plt.subplot(1,2,2)
plt.plot(hist['tr_acc'], label='train_acc'); plt.plot(hist['val_acc'], label='val_acc')
plt.xlabel('epoch'); plt.title('Accuracy'); plt.legend()
plt.tight_layout()
plt.savefig(OUTDIR/"training_curves.png", dpi=150)
plt.show()

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(5,4))
plt.imshow(cm, cmap='Blues'); plt.colorbar()
plt.xticks([0,1], ["down","up"]); plt.yticks([0,1], ["down","up"])
plt.title(f"Confusion matrix (test)  acc={acc_lstm:.4f}")
plt.tight_layout()
plt.savefig(OUTDIR/"confusion_matrix.png", dpi=150)
plt.show()

# 12) Save predictions and model weights
# build test dates aligned with seq shift
test_dates = test_df.index[SEQ_LEN:]  # because seq dataset shifts by SEQ_LEN
if len(test_dates) != len(y_true):
    # fallback: create generic index
    test_dates = pd.date_range(start=test_df.index[0], periods=len(y_true), freq='D')
pred_df = pd.DataFrame({"date": test_dates, "y_true": y_true, "y_pred": y_pred})
pred_df.set_index('date', inplace=True)
pred_df.to_csv(OUTDIR/"test_predictions.csv")
torch.save(model.state_dict(), OUTDIR/"final_btc_lstm.pth")
print("Saved outputs to", OUTDIR)

# 13) Quick visualization: price vs predicted up days
plt.figure(figsize=(12,4))
# align predictions to original index: predictions correspond to dates starting at test_df.index[SEQ_LEN]
price_series = test_df['Adj_Close'].copy()
pred_dates = pred_df.index
plt.plot(price_series.loc[pred_dates.min():pred_dates.max()], label='BTC Adj Close')
# mark predicted up days
up_idx = pred_df[pred_df['y_pred']==1].index
plt.scatter(up_idx, price_series.loc[up_idx], marker='^', color='green', label='pred up', zorder=3)
plt.title('BTC price & predicted up-days (test period)')
plt.legend(); plt.tight_layout(); plt.savefig(OUTDIR/"price_with_preds.png", dpi=150); plt.show()
